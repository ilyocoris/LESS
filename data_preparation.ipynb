{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm as tqdm\n",
    "from datasets import load_dataset\n",
    "from datasets import inspect_dataset, load_dataset_builder\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 411/411 [00:00<00:00, 1.89MB/s]\n",
      "Downloading data: 100%|██████████| 444k/444k [00:00<00:00, 489kB/s]\n",
      "Generating train split: 100%|██████████| 1418/1418 [00:00<00:00, 76053.34 examples/s]\n",
      "Downloading readme: 100%|██████████| 411/411 [00:00<00:00, 1.55MB/s]\n",
      "Downloading data: 100%|██████████| 420k/420k [00:00<00:00, 511kB/s]\n",
      "Generating train split: 100%|██████████| 1418/1418 [00:00<00:00, 160791.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "enzh = load_dataset(\"yezhengli9/wmt20-en-zh\")\n",
    "ende = load_dataset(\"yezhengli9/wmt20-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"data/translation-less\"\n",
    "for name, data in zip([\"ende\", \"enzh\"], [ende, enzh]):\n",
    "    dataset_jsonl = []\n",
    "    source_lang = name[:2]\n",
    "    target_lang = name[2:]\n",
    "    for i,sample in enumerate(data[\"train\"]):\n",
    "        translation = json.loads(sample[\"translation (translation)\"])\n",
    "        dataset_jsonl.append(\n",
    "            {\n",
    "                \"dataset\": f\"wmt20_{name}\",\n",
    "                \"id\": f\"{name}_{i}\",\n",
    "                \"messages\":[\n",
    "                    {\n",
    "                        \"role\":\"user\",\n",
    "                        \"content\":f\"Translate this sentence from {source_lang} to {target_lang}: {translation[source_lang]}\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\":\"assistant\",\n",
    "                        \"content\":f\"{translation[target_lang]}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    with open(f\"{save_path}/{name}.jsonl\", \"w\") as f:\n",
    "        for line in dataset_jsonl:\n",
    "            f.write(json.dumps(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'cot',\n",
       " 'id': 'cot_0',\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'Test for natural language inference.\\nPremise: \"A costume party in full swing.\"\\nHypothesis: \"Everyone at the party is in regular clothes.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell Step by step answer:'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Everyone is either in a costume or regular clothes at a party. Therefore, the final answer is no.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"data/less-data/train\")\n",
    "dataset[\"train\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "less",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
