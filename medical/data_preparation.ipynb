{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1428 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1428/1428 [00:26<00:00, 53.20it/s]\n",
      "100%|██████████| 2783/2783 [00:52<00:00, 52.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_page_text(text: str):\n",
    "    # sub \\ue01 and \\n\n",
    "    text = re.sub(\"\\ue04a|\\ue04b|\\ue01e|-\\n\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    # split into sentences\n",
    "    sentences = split_text_into_sentences(text, language=\"en\")\n",
    "    # remove short sentences\n",
    "    sentences = [s for s in sentences if len(s) > 10]\n",
    "    return sentences\n",
    "\n",
    "pdfs = ['../data/out/medical/datasets/medbook.pdf','../data/out/medical/datasets/medbook2.pdf']\n",
    "pages = []\n",
    "for pdf_path in pdfs:\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Iterate over each page in the document\n",
    "    for page in tqdm(doc):\n",
    "        clean_page = clean_page_text(page.get_text())\n",
    "        if len(clean_page) > 10:\n",
    "            # first line is usually noisy with the page header or the end of a sentence from the previous page\n",
    "            pages.append(clean_page[1:])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each page, divide it prompt + completion splits where the pormpt is 1 sentence and the completion is 3 sentences or 4 sentences (0.85 prob split)\n",
    "train_jsonl = []\n",
    "for page in pages:\n",
    "    for i in range(len(page)-4):\n",
    "        prompt = page[i]\n",
    "        completion_size = random.choice([3,3,4,5,6])\n",
    "        completion = \" \".join(page[i+1:i+1+completion_size])\n",
    "        train_jsonl.append({'prompt': prompt, 'completion': completion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open('../data/out/medical/train.jsonl', 'w') as f:\n",
    "    for item in train_jsonl:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146927/146927 [08:10<00:00, 299.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sentences: 1.0\n",
      "Completion sentences: 4.178156499486139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_average_lengths(dataset_jsonl):\n",
    "    prompt_sentences = []\n",
    "    completion_sentences = []\n",
    "    for example in tqdm(dataset_jsonl):\n",
    "        prompt_sentences.append(len(split_text_into_sentences(example['prompt'], language=\"en\")))\n",
    "        completion_sentences.append(len(split_text_into_sentences(example['completion'], language=\"en\")))\n",
    "    print(f\"Prompt sentences: {sum(prompt_sentences)/len(prompt_sentences)}\")\n",
    "    print(f\"Completion sentences: {sum(completion_sentences)/len(completion_sentences)}\")\n",
    "\n",
    "get_average_lengths(train_jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medalpaca/medical_meadow_medical_flashcards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33955/33955 [01:40<00:00, 336.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sentences: 0.9972610808422913\n",
      "Completion sentences: 2.691032248564276\n",
      "Processing medalpaca/medical_meadow_wikidoc_patient_information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5942/5942 [00:18<00:00, 327.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sentences: 1.000673174015483\n",
      "Completion sentences: 4.819757657354426\n",
      "Processing monology/medical_meadow_alpaca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5942/5942 [00:18<00:00, 328.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sentences: 1.000673174015483\n",
      "Completion sentences: 4.819757657354426\n",
      "Processing medalpaca/medical_meadow_wikidoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:35<00:00, 282.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sentences: 1.0238\n",
      "Completion sentences: 6.6465\n"
     ]
    }
   ],
   "source": [
    "# prepare the medical test set\n",
    "dev_path = \"../data/out/medical/datasets/\"\n",
    "\n",
    "medical_qa_datasets = [\n",
    "    \"medalpaca/medical_meadow_medical_flashcards\",\n",
    "    \"medalpaca/medical_meadow_wikidoc_patient_information\",\n",
    "    \"medalpaca/medical_meadow_wikidoc\"\n",
    "]\n",
    "\n",
    "\n",
    "for dataset_name in medical_qa_datasets:\n",
    "    dataset_jsonl = []\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    for example in dataset['train']:\n",
    "        dataset_jsonl.append({\n",
    "            \"prompt\": example['input'],\n",
    "            \"completion\": example['output'],\n",
    "        })\n",
    "    get_average_lengths(dataset_jsonl)\n",
    "    with open(dev_path+\"/\"+dataset_name.split(\"/\")[1]+\".jsonl\", 'w') as f:\n",
    "        for example in dataset_jsonl:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "less",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
